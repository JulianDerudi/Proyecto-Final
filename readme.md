# Proyecto Final â€“ IngenierÃ­a de Datos

### ETL desde API pÃºblica hacia Base de Datos MySQL

Este proyecto corresponde al **trabajo final del curso de IngenierÃ­a de Datos (UTN)**.
Su objetivo principal es realizar un proceso **ETL completo (Extract â€“ Transform â€“ Load)** tomando datos desde una **API pÃºblica**, procesÃ¡ndolos y almacenÃ¡ndolos en una **base de datos relacional**.

---

## ğŸ§© Objetivo del Proyecto

El propÃ³sito fue construir un pipeline capaz de:

1. **Extraer** informaciÃ³n desde una API externa.
2. **Transformar** y limpiar los datos (normalizaciÃ³n, parseo de tipos, validaciones, eliminaciÃ³n de duplicados, etc.).
3. **Cargar** la informaciÃ³n procesada en una base de datos **MySQL**, lista para ser consultada o utilizada por otros sistemas.

Este proyecto replica un flujo real utilizado en entornos de **Data Engineering**.

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

* **Python 3**
* **Requests** (consumo de API)
* **Pandas** (limpieza y transformaciÃ³n de datos)
* **MySQL / MySQL Connector**
* **dotenv** (manejo seguro de credenciales)
* **Jupyter Notebook** (desarrollo y documentaciÃ³n del proceso)

---

## ğŸ“¡ Fuente de Datos (API)

El proyecto se conecta a una API pÃºblica (detallada en el notebook) desde donde se obtienen los datos crudos a procesar.

---

## ğŸ”„ Flujo ETL Implementado

### **1. ExtracciÃ³n**

* Se realiza una solicitud HTTP a la API.
* Se validan respuestas, cÃ³digos de estado y estructura del JSON.
* Los datos crudos se convierten en un DataFrame inicial.

### **2. TransformaciÃ³n**

* NormalizaciÃ³n de campos.
* ConversiÃ³n de tipos numÃ©ricos y fechas.
* Limpieza de registros invÃ¡lidos.
* EliminaciÃ³n de duplicados.
* EstructuraciÃ³n final para ajustarse al modelo relacional de la base de datos.

### **3. Carga**

* ConexiÃ³n a MySQL mediante credenciales protegidas por `.env`.
* CreaciÃ³n de tablas (si no existen).
* InserciÃ³n de datos transformados.

---

## ğŸ§± Estructura del Repositorio

```
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Proyecto_Final.ipynb     # Notebook principal con todo el proceso ETL
â”œâ”€â”€ src/
â”‚   â””â”€â”€ etl.py                   # Script modular del pipeline (si aplica)
â”œâ”€â”€ .env.example                 # Ejemplo de variables de entorno
â”œâ”€â”€ requirements.txt             # Dependencias del proyecto
â””â”€â”€ README.md                    # Este archivo
```

---

## ğŸ“Š Modelo de Datos

El script/notebook genera una base de datos con tablas normalizadas segÃºn los datos obtenidos de la API.

*(PodÃ©s agregar aquÃ­ un diagrama ER si lo tenÃ©s.)*

---

## ğŸš€ CÃ³mo Ejecutarlo

### **1. Clonar el repositorio**

```
git clone https://github.com/JulianDerudi/Proyecto-Final.git
cd Proyecto-Final
```

### **2. Crear y configurar el archivo `.env`**

```
DB_HOST=localhost
DB_USER=tu_usuario
DB_PASSWORD=tu_password
DB_NAME=nombre_bd
```

### **3. Instalar dependencias**

```
pip install -r requirements.txt
```

### **4. Ejecutar el ETL**

PodÃ©s hacerlo desde Jupyter Notebook o el script Python mÃ³dulo ETL (si lo incluÃ­s).

---

## ğŸ“š Aprendizajes del Proyecto

* Manejo de datos desde APIs reales.
* ConstrucciÃ³n de pipelines ETL.
* NormalizaciÃ³n y limpieza de datasets.
* ConexiÃ³n y almacenamiento en MySQL.
* Buenas prÃ¡cticas con entorno virtual y manejo de credenciales.

---

## ğŸ“ Contacto

**JuliÃ¡n Derudi**
Portafolio: [https://julianderudi.github.io/Portafolio/](https://julianderudi.github.io/Portafolio/)
LinkedIn: [https://www.linkedin.com/in/julian-derudi-730ba8343/](https://www.linkedin.com/in/julian-derudi-730ba8343/)

---

Si te resulta Ãºtil o querÃ©s colaborar, Â¡no dudes en abrir un issue!
